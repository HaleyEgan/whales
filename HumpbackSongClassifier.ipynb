{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "817aec78-89ef-45b3-9dd1-4e41aa142859",
   "metadata": {},
   "source": [
    "### Humpback whales acoustic detector (by NOAA & Google)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17058db6-b4bf-4b8c-bec0-afcfef09e53e",
   "metadata": {},
   "source": [
    "#### Song classifier using CNN\n",
    "- classify audio segments as containing or not containing humpback whale sounds\n",
    "- \"intended to be applied as a detector by scoring every context window (3.92 sec) in a set of underwater passive acoustic monitoring data.\"\n",
    "- PCEN-normalized spectrogram -> ResNet-50 -> single logistic output unit\n",
    "- original study from sounds collected in Hawaiian archipelago of humpback winter breeding grounds.\n",
    "- Metric: score\n",
    "    - \"scores batches of waveforms at once, framing each waveform in the batch into multiple context windows before outputting per-window scores.\"\n",
    "- HARP used to collect original data https://ieeexplore.ieee.org/document/4231090 that was used to train model. deployed at hundreds of meters under water (reduces noise disturbance)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5126809d-6e61-4f81-9328-1dc0dda494c7",
   "metadata": {},
   "source": [
    "#### Use cases\n",
    "This model is suitable for:\n",
    "- Predicting the presence of a humpback whale call in a given audio sample.\n",
    "- Analyzing acoustic data collected by deep-water deployments.\n",
    "\n",
    "This model is NOT suitable for:\n",
    "- Detecting species of whales other than humpback whales.\n",
    "- Counting how many whales are present.\n",
    "- Localizing whales.\n",
    "- Analyzing acoustic data with high levels of surface or platform noise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0579b517-1f2d-4bca-93ac-45ed48d53b4f",
   "metadata": {},
   "source": [
    "Dataset: https://data.noaa.gov/metaview/page?xml=NOAA/NESDIS/NGDC/MGG/passive_acoustic//iso/xml/PIFSC_HARP_10kHzDecimated.xml&view=getDataView\n",
    "\n",
    "https://console.cloud.google.com/storage/browser/noaa-passive-bioacoustic/pifsc;tab=objects?prefix=&forceOnObjectsSortingFiltering=false"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c7069a-1243-4ac1-a999-66d1df273a41",
   "metadata": {},
   "source": [
    "A. Allen et al., \"A convolutional neural network for automated detection of humpback whale song in a diverse, long-term passive acoustic dataset\", Front. Mar. Sci., 2021, doi: 10.3389/fmars.2021.607321.\n",
    "\n",
    "M. Harvey, \"Acoustic Detection of Humpback Whales Using a Convolutional Neural Network,\" Google AI Blog, Oct. 29, 2018."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d127884e-d246-488a-95a0-f7e0ac010761",
   "metadata": {},
   "source": [
    "##### Code Synopsis\n",
    "This code imports the TensorFlow and TensorFlow Hub libraries. It then loads a pre-trained machine learning model for audio classification from TensorFlow Hub, specifically the Humpback Whale Classification model with a version number of 1. The code then reads a WAV audio file from a Google Cloud Storage bucket and decodes it into a waveform tensor and its sample rate. The waveform tensor is then reshaped into a batch of size 1, and the sample rate is cast into an integer tensor. The model's 'score' signature is then obtained and used to make predictions on the waveform tensor with the given sample rate, resulting in scores which are then printed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5787566-43a6-4593-b301-8a357ed2b9e8",
   "metadata": {},
   "source": [
    "#### Inputs\n",
    "- waveform, a float32 Tensor of shape [batch_size, num_samples, num_channels], where it is required that num_channels = 1, but where batch_size and num_samples may take the caller's preferred values on each call.\n",
    "    - Each audio channel (slice [channel_index, :, 0]), should contain 10kHz PCM float32 audio.\n",
    "        - The training data left plenty of headroom; the level of clips with humpback present was typically 0.003 RMS, 0.02 peak, much \"quieter\" than consumer digital audio.\n",
    "        - Although the model is relatively insensitive to input gain variations as wide as +/-20 dB, users may wish to apply linear scaling to match the levels the model saw in training.\n",
    "- context_samples, an int64 Tensor of shape [], the hop length at which to slide the scoring context window over waveform.\n",
    "### Outputs\n",
    "- scores, a float32 Tensor of shape [batch_size, num_windows, num_classes], where it will always be true that num_classes = 1, where batch_size will equal the one from the input, and where num_windows is determined by num_samples and context_step_samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e6a3bac-c38b-427c-b021-48c77f1a5a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install protobuf==3.20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eee7102c-0976-42b3-83cd-7745aa74ba9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub #contains reusable/pre-trained models\n",
    "import tensorflow.compat.v1 as tf #allows access the TensorFlow 1.x API while running their code on TensorFlow 2.x,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6d15d8b-1cfb-4182-b35a-65815b3b6ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'scores': <tf.Tensor: shape=(1, 72, 1), dtype=float32, numpy=\n",
      "array([[[0.90622807],\n",
      "        [0.95901173],\n",
      "        [0.8324897 ],\n",
      "        [0.9507026 ],\n",
      "        [0.8979412 ],\n",
      "        [0.86508614],\n",
      "        [0.9822989 ],\n",
      "        [0.8624318 ],\n",
      "        [0.7249464 ],\n",
      "        [0.37265933],\n",
      "        [0.990205  ],\n",
      "        [0.9814316 ],\n",
      "        [0.98871076],\n",
      "        [0.99518096],\n",
      "        [0.97144973],\n",
      "        [0.7024987 ],\n",
      "        [0.99146575],\n",
      "        [0.9982121 ],\n",
      "        [0.9950129 ],\n",
      "        [0.9127229 ],\n",
      "        [0.9984227 ],\n",
      "        [0.995862  ],\n",
      "        [0.9872175 ],\n",
      "        [0.95618635],\n",
      "        [0.98510885],\n",
      "        [0.9986504 ],\n",
      "        [0.9936776 ],\n",
      "        [0.99768335],\n",
      "        [0.9981987 ],\n",
      "        [0.9946393 ],\n",
      "        [0.9704775 ],\n",
      "        [0.9052821 ],\n",
      "        [0.96297425],\n",
      "        [0.92693096],\n",
      "        [0.9818676 ],\n",
      "        [0.9975908 ],\n",
      "        [0.9129576 ],\n",
      "        [0.9471698 ],\n",
      "        [0.74529874],\n",
      "        [0.93864006],\n",
      "        [0.88221884],\n",
      "        [0.986338  ],\n",
      "        [0.99153847],\n",
      "        [0.981916  ],\n",
      "        [0.99406624],\n",
      "        [0.99691904],\n",
      "        [0.99704266],\n",
      "        [0.9803993 ],\n",
      "        [0.97919637],\n",
      "        [0.9901272 ],\n",
      "        [0.9623955 ],\n",
      "        [0.89800626],\n",
      "        [0.8192487 ],\n",
      "        [0.9253861 ],\n",
      "        [0.9975072 ],\n",
      "        [0.992026  ],\n",
      "        [0.9913523 ],\n",
      "        [0.94039077],\n",
      "        [0.99342865],\n",
      "        [0.97715175],\n",
      "        [0.97083884],\n",
      "        [0.94766253],\n",
      "        [0.7955562 ],\n",
      "        [0.9989901 ],\n",
      "        [0.98586893],\n",
      "        [0.9994473 ],\n",
      "        [0.99779236],\n",
      "        [0.9993486 ],\n",
      "        [0.9994341 ],\n",
      "        [0.99270725],\n",
      "        [0.91305083],\n",
      "        [0.97207856]]], dtype=float32)>}\n"
     ]
    }
   ],
   "source": [
    "FILENAME = 'gs://bioacoustics-www1/sounds/Cross_02_060203_071428.d20_7.wav' #file as string\n",
    "model = hub.load('https://tfhub.dev/google/humpback_whale/1') #load model\n",
    "\n",
    "#decode WAV audio file\n",
    "#reads WAV file 'FILENAME' with 'tf.io.read_file()', then passes contents to 'tf.audio.decode_wav()' to decode\n",
    "#output is a tensor (multidimensional matrix that stores data & performs computations. represents output of data) with audio data\n",
    "waveform, sample_rate = tf.audio.decode_wav(tf.io.read_file(FILENAME))\n",
    "waveform = tf.expand_dims(waveform, 0) #makes a batch of size 1\n",
    "context_step_samples = tf.cast(sample_rate, tf.int64) #'tf.cast' converts 'sample_rate' from float to integer\n",
    "\n",
    "#access 'score' function by using key 'score' in the 'signatures' dictionary, from 'model' object in pre-trained model\n",
    "#can run 'score' function on inputs (audio data) to return score/prediction (specific to pre-trained model)\n",
    "score_fn = model.signatures['score']\n",
    "\n",
    "#'waveform' = tensor that represents audio data\n",
    "#'context_step_samples'= integer that represents number of samples in a context window\n",
    "scores = score_fn(waveform=waveform, context_step_samples=context_step_samples)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1b3bf7ec-5e4d-4eab-a45f-95607eabc4a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'scores': array([[[0.90622807],\n",
      "        [0.95901173],\n",
      "        [0.8324897 ],\n",
      "        [0.9507026 ],\n",
      "        [0.8979412 ],\n",
      "        [0.86508614],\n",
      "        [0.9822989 ],\n",
      "        [0.8624318 ],\n",
      "        [0.7249464 ],\n",
      "        [0.37265933],\n",
      "        [0.990205  ],\n",
      "        [0.9814316 ],\n",
      "        [0.98871076],\n",
      "        [0.99518096],\n",
      "        [0.97144973],\n",
      "        [0.7024987 ],\n",
      "        [0.99146575],\n",
      "        [0.9982121 ],\n",
      "        [0.9950129 ],\n",
      "        [0.9127229 ],\n",
      "        [0.9984227 ],\n",
      "        [0.995862  ],\n",
      "        [0.9872175 ],\n",
      "        [0.95618635],\n",
      "        [0.98510885],\n",
      "        [0.9986504 ],\n",
      "        [0.9936776 ],\n",
      "        [0.99768335],\n",
      "        [0.9981987 ],\n",
      "        [0.9946393 ],\n",
      "        [0.9704775 ],\n",
      "        [0.9052821 ],\n",
      "        [0.96297425],\n",
      "        [0.92693096],\n",
      "        [0.9818676 ],\n",
      "        [0.9975908 ],\n",
      "        [0.9129576 ],\n",
      "        [0.9471698 ],\n",
      "        [0.74529874],\n",
      "        [0.93864006],\n",
      "        [0.88221884],\n",
      "        [0.986338  ],\n",
      "        [0.99153847],\n",
      "        [0.981916  ],\n",
      "        [0.99406624],\n",
      "        [0.99691904],\n",
      "        [0.99704266],\n",
      "        [0.9803993 ],\n",
      "        [0.97919637],\n",
      "        [0.9901272 ],\n",
      "        [0.9623955 ],\n",
      "        [0.89800626],\n",
      "        [0.8192487 ],\n",
      "        [0.9253861 ],\n",
      "        [0.9975072 ],\n",
      "        [0.992026  ],\n",
      "        [0.9913523 ],\n",
      "        [0.94039077],\n",
      "        [0.99342865],\n",
      "        [0.97715175],\n",
      "        [0.97083884],\n",
      "        [0.94766253],\n",
      "        [0.7955562 ],\n",
      "        [0.9989901 ],\n",
      "        [0.98586893],\n",
      "        [0.9994473 ],\n",
      "        [0.99779236],\n",
      "        [0.9993486 ],\n",
      "        [0.9994341 ],\n",
      "        [0.99270725],\n",
      "        [0.91305083],\n",
      "        [0.97207856]]], dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "#create tensorflow graph \n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    model = hub.load('https://tfhub.dev/google/humpback_whale/1')\n",
    "    \n",
    "    filename = tf.placeholder(tf.string)\n",
    "    waveform, sample_rate = tf.audio.decode_wav(tf.io.read_file(filename))\n",
    "    \n",
    "    waveform = tf.expand_dims(waveform, 0)\n",
    "    context_step_samples = tf.cast(sample_rate, tf.int64)\n",
    "    score_fn = model.signatures['score']\n",
    "    scores = score_fn(waveform=waveform, context_step_samples=context_step_samples)\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    feed_dict = {filename: FILENAME}\n",
    "    print(sess.run(scores, feed_dict))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0666987-b972-41ce-b495-d438db4529d3",
   "metadata": {},
   "source": [
    "#### Advanced Usage\n",
    "Model attributes allow isolated reuse of parts of the model, in accord with the Reusable SavedModels interface. The callable attributes exposed are:\n",
    "\n",
    "- front_end, which can be called on a waveform Tensor as described in the score signature inputs to produce a PCEN-normalized spectrogram of shape [batch_size, num_stft_bins, num_channels], where num_channels = 64 is fixed and where num_stft_bins depends on the number of input samples.\n",
    "- features, which when called on a PCEN spectrogram slice of shape [batch_size, 128, 64] produces feature vectors of shape [batch_size, 2048]. (These might be useful for detecting other audio event types in the HARP data or similar underwater passive acoustic monitoring datasets, but the model developers have not yet validated this through experiment.)\n",
    "- logits, which, when called on the same type of input as features, outputs the log odds of the input spectrogram containing humpback vocalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae21f4e-4190-4341-bb1a-45dc412647f3",
   "metadata": {},
   "source": [
    "This code uses TensorFlow and TensorFlow Hub to classify an audio file of a whale call. The audio file is located in a Google Cloud Storage bucket and is specified by the FILENAME constant.\n",
    "\n",
    "The code first loads a pre-trained model from TensorFlow Hub by using the hub.load() function and passing in the URL of the model. Then, it decodes the WAV audio file into a tensor using tf.audio.decode_wav() and expands its dimension with tf.expand_dims() so it has a batch size of 1.\n",
    "\n",
    "The code then passes the audio tensor through the front-end and features functions of the pre-trained model to extract logits, which are used to make predictions about the audio. The logits are passed through the sigmoid function to obtain probabilities between 0 and 1.\n",
    "\n",
    "Finally, the code prints out a dictionary containing the intermediate results of the computations, including the pcen spectrogram, features, logits, and probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "90a447b6-6360-472b-8a40-584f5e11f7a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pcen_spectrogram': <tf.Tensor: shape=(1, 2497, 64), dtype=float32, numpy=\n",
      "array([[[0.34337044, 0.35289788, 0.3364389 , ..., 0.3155048 ,\n",
      "         0.30421436, 0.3076836 ],\n",
      "        [0.01286328, 0.11843169, 0.3996929 , ..., 0.3654033 ,\n",
      "         0.38193524, 0.3762566 ],\n",
      "        [0.02342975, 0.06558263, 0.18811762, ..., 0.48939443,\n",
      "         0.47875965, 0.29559004],\n",
      "        ...,\n",
      "        [0.14307141, 0.222906  , 0.5149369 , ..., 0.16156292,\n",
      "         0.25384068, 0.39627314],\n",
      "        [0.1394161 , 0.0664804 , 0.13821816, ..., 0.22090197,\n",
      "         0.30796683, 0.3320781 ],\n",
      "        [0.1844101 , 0.17328942, 0.07394493, ..., 0.21915352,\n",
      "         0.3080541 , 0.18283725]]], dtype=float32)>, 'features': <tf.Tensor: shape=(1, 2048), dtype=float32, numpy=\n",
      "array([[1.6633592, 0.7753173, 1.2360735, ..., 2.1375275, 1.1309164,\n",
      "        2.763502 ]], dtype=float32)>, 'logits': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[2.268426]], dtype=float32)>, 'probabilities': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.9062281]], dtype=float32)>}\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "FILENAME = 'gs://bioacoustics-www1/sounds/Cross_02_060203_071428.d20_7.wav'\n",
    "\n",
    "model = hub.load('https://tfhub.dev/google/humpback_whale/1')\n",
    "\n",
    "waveform, _ = tf.audio.decode_wav(tf.io.read_file(FILENAME))\n",
    "waveform = tf.expand_dims(waveform, 0)# makes a batch of size 1\n",
    "\n",
    "pcen_spectrogram = model.front_end(waveform)\n",
    "context_window = pcen_spectrogram[:, :128, :]\n",
    "features = model.features(context_window)\n",
    "logits = model.logits(context_window)\n",
    "probabilities = tf.nn.sigmoid(logits)\n",
    "\n",
    "print({\n",
    "    'pcen_spectrogram': pcen_spectrogram,\n",
    "    'features': features,\n",
    "    'logits': logits,\n",
    "    'probabilities': probabilities,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1ac52c30-5515-488c-9476-e1268e0c7b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pcen_spectrogram': array([[[0.34337044, 0.35289788, 0.3364389 , ..., 0.3155048 ,\n",
      "         0.30421436, 0.3076836 ],\n",
      "        [0.01286328, 0.11843169, 0.3996929 , ..., 0.3654033 ,\n",
      "         0.38193524, 0.3762566 ],\n",
      "        [0.02342975, 0.06558263, 0.18811762, ..., 0.48939443,\n",
      "         0.47875965, 0.29559004],\n",
      "        ...,\n",
      "        [0.14307141, 0.222906  , 0.5149369 , ..., 0.16156292,\n",
      "         0.25384068, 0.39627314],\n",
      "        [0.1394161 , 0.0664804 , 0.13821816, ..., 0.22090197,\n",
      "         0.30796683, 0.3320781 ],\n",
      "        [0.1844101 , 0.17328942, 0.07394493, ..., 0.21915352,\n",
      "         0.3080541 , 0.18283725]]], dtype=float32), 'features': array([[1.6633592, 0.7753173, 1.2360735, ..., 2.1375275, 1.1309164,\n",
      "        2.763502 ]], dtype=float32), 'logits': array([[2.268426]], dtype=float32), 'probabilities': array([[0.9062281]], dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "#cleaned up print out\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "FILENAME = 'gs://bioacoustics-www1/sounds/Cross_02_060203_071428.d20_7.wav'\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    model = hub.load('https://tfhub.dev/google/humpback_whale/1')\n",
    "    filename = tf.placeholder(tf.string)\n",
    "    waveform, _ = tf.audio.decode_wav(tf.io.read_file(filename))\n",
    "    waveform = tf.expand_dims(waveform, 0)# makes a batch of size 1\n",
    "    \n",
    "    pcen_spectrogram = model.front_end(waveform)\n",
    "    context_window = pcen_spectrogram[:, :128, :]\n",
    "    features = model.features(context_window)\n",
    "    logits = model.logits(context_window)\n",
    "    probabilities = tf.nn.sigmoid(logits)\n",
    "    \n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    feed_dict = {filename: FILENAME}\n",
    "    print(\n",
    "        sess.run(\n",
    "            {\n",
    "                'pcen_spectrogram': pcen_spectrogram,\n",
    "                'features': features,\n",
    "                'logits': logits,\n",
    "                'probabilities': probabilities,\n",
    "            }, feed_dict))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11abf8e3-5028-481d-9ca6-03c4f70b9c21",
   "metadata": {},
   "source": [
    "The metadata signature returns the sample rate of the audio the model expects to see as input and the duration of the context window to which each score applies. This signature is a bit of future proofing so that batch inference systems can support models where these values may differ.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "db00fdc1-78c3-4b69-af2f-cf540555cae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_sample_rate': <tf.Tensor: shape=(), dtype=int64, numpy=10000>, 'context_width_samples': <tf.Tensor: shape=(), dtype=int64, numpy=39124>, 'class_names': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'Mn'], dtype=object)>}\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_hub as hub\n",
    "\n",
    "model = hub.load('https://tfhub.dev/google/humpback_whale/1')\n",
    "metadata_fn = model.signatures['metadata']\n",
    "metadata = metadata_fn()\n",
    "print(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c74c9433-2195-4e48-a6c2-925f9b10309f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'context_width_samples': 39124, 'input_sample_rate': 10000, 'class_names': array([b'Mn'], dtype=object)}\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    model = hub.load('https://tfhub.dev/google/humpback_whale/1')\n",
    "    metadata_fn = model.signatures['metadata']\n",
    "    metadata = metadata_fn()\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    print(sess.run(metadata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb29027c-1dfb-40fd-99d2-5f052550a6ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
