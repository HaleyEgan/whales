{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30d3ea83-7933-428d-a11b-61c2eb7d884a",
   "metadata": {},
   "source": [
    "### Autoencoder on Image Data\n",
    "- unsupervised encoder-decoder model for preprocessing images\n",
    "- reduce noise, anomaly detection, image generation \n",
    "- Resources: https://www.tensorflow.org/tutorials/generative/autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df708728-5fb4-4172-997e-2e4304998594",
   "metadata": {},
   "source": [
    "An autoencoder is a special type of neural network that is trained to copy its input to its output. For example, given an image of a handwritten digit, an autoencoder first encodes the image into a lower dimensional latent representation, then decodes the latent representation back to an image. An autoencoder learns to compress the data while minimizing the reconstruction error.\n",
    "\n",
    "Adding noise to the input images and training the autoencoder to compare it to the original image serves as a form of regularization that helps prevent overfitting and improves the generalization performance of the model.\n",
    "\n",
    "When an autoencoder is trained without any noise in the input, it can learn to simply memorize the input images and their corresponding reconstructions. This can lead to overfitting, where the model performs well on the training data but poorly on new, unseen data.\n",
    "\n",
    "By adding noise to the input images, the autoencoder is forced to learn a more robust and generalizable representation of the data. Specifically, the encoder network must learn to extract the underlying features and patterns from the noisy input, while the decoder network must learn to reconstruct the original image from this noisy representation. This can help the model generalize better to new, unseen images, even if they have some level of noise or imperfections.\n",
    "\n",
    "Additionally, adding noise to the input images can help prevent the autoencoder from simply copying the input to the output, which can happen if the model is too powerful or if the dataset is too simple. By requiring the model to perform some form of reconstruction or denoising, we can encourage it to learn more useful and meaningful features of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd0411ec-a54c-419e-9d3f-3e893e389bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers, losses\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74247355-fa6d-4d6b-ae60-3b966dd4d904",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
